{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HW4.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JakeCode11/HW4/blob/master/HW4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oMiFUmseKw8z",
        "colab_type": "text"
      },
      "source": [
        "### **General Concepts**\n",
        "\n",
        "**Artificial Intellgience:**\n",
        "\n",
        "\n",
        "\n",
        "> Artificial intelligence (AI) is one of the research fields and theoretical applications of programming that has, in recent years, become a focal point of discussion. AI is a step up above former programming techniques as it proposes the idea that computers can perform more sophisticated and complex tasks.\n",
        "\n",
        "> In essence, AI is the ability for a computer system to learn and perform tasks that normally require human intelligence. Such tasks include, but are not limited to, visual perception, speech recognition, decision-making, and translation between languages.\n",
        "\n",
        "> On a foundational level, AI takes in an input and set of rules to compute an output.\n",
        "\n",
        "**Machine Learning:**\n",
        "\n",
        "> As the name implies, Machine Learning (ML) takes a similar, but also different, approach to how AI solves problems. ML algorithms or programs adjust themselves in response to new data that they are exposed to. Additionally, ML is more of a dynamic experience in which very limited human intervention is required to make specific changes during the developmental process.\n",
        "\n",
        "> ML algorithms are fascinating in that they are able to modify themselves when exposed to more data during its lifetime. ML is different from AI in the sense that in its initialization, the algorithm is only aware of the input and output. The goal of the ML algorithm is to determine the rules or parameters to generate or determine the intended output from the given input."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "83-fAj2GQrQh",
        "colab_type": "text"
      },
      "source": [
        "### **Building a Model**\n",
        "\n",
        "\n",
        "> When building a model, there are a number of variables and levels of understanding to consider to ensure that your output is the output you intended to find from analyzing a certain set of data.\n",
        "\n",
        "> Some things to consider when building the model are the following:\n",
        "*   Choosing a Measure of Success\n",
        "*   Setting up an Evalutation Protocol\n",
        "*   Preparing the data that you have chosen prior to training\n",
        "\n",
        "> With these in mind, it makes it a more streamline experience and provides better structure to attempting to solve a novel problem within computer science.\n",
        "\n",
        "> One very popular model that many computer scientists use frequently is linear regression. Linear regression in essence generates a 'line of best fit' for a series of data and utilizes that 'line' to predict the next outcome for a given arbitrary value. Below is an example of what a linear regression model looks like.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mu0UK7J2KtR6",
        "colab_type": "code",
        "outputId": "858cea64-3f80-400e-a9ae-b6f275958ffe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 225
        }
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "# x from 0 to 30\n",
        "x = 30 * np.random.random((20, 1))\n",
        "\n",
        "# y = a*x + b with noise\n",
        "y = 0.5 * x + 1.0 + np.random.normal(size=x.shape)\n",
        "\n",
        "# create a linear regression model\n",
        "model = LinearRegression()\n",
        "model.fit(x, y)\n",
        "\n",
        "# predict y from the data\n",
        "x_new = np.linspace(0, 30, 100)\n",
        "y_new = model.predict(x_new[:, np.newaxis])\n",
        "\n",
        "# plot the results\n",
        "plt.figure(figsize=(4, 3))\n",
        "ax = plt.axes()\n",
        "ax.scatter(x, y)\n",
        "ax.plot(x_new, y_new)\n",
        "ax.set_xlabel('x')\n",
        "ax.set_ylabel('y')\n",
        "ax.axis('tight')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQ8AAADQCAYAAAAZMORwAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 288x216 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0kMCoydjT_Kx",
        "colab_type": "text"
      },
      "source": [
        "### **Compiling a Model**\n",
        "\n",
        "> In the compilation process of a model, it is important to set up the model with the adequate parameters to run at the most optimized performance to yield accurate results for the intended dataset.\n",
        "\n",
        "> The following code is a good outline from my Homework 3 that shows how to define the different parameters of a model to structure it to better produce the intended output during the validation phase of the algorithm.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M1Ie2WltXJb0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learning_rate = 0.0002 \n",
        "lr_decay = 0.0001\n",
        "batch = 64\n",
        "fcLayer1 = 32\n",
        "dropout = 0.5\n",
        "\n",
        "epochsL1 = 10\n",
        "patiencel1 = 1\n",
        "factorL1 = 0.5\n",
        "\n",
        "epochsL2 = 10\n",
        "patiencel2 = 1\n",
        "factorL2 = 0.5\n",
        "\n",
        "verbose_train = 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TkLvUfshXMvN",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "> From the code above, each of these variables contributes to the overall structure of the model and affects how thoroughly, efficiently, and quickly the data will be computed through the model a programmer designed. As there are numerous types of models and approaches to structuring the models, these initial parameters in the compilation process are a very pivotal and important aspect to generating accurate ouput.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xm8qurwgXvTq",
        "colab_type": "text"
      },
      "source": [
        "### **Training a Model**\n",
        "\n",
        "> Training a model is one of the most important parts of the AI/ML developmental life cycle. It gives the programmer a chance to observe any significant changes when new, unknown data is put through the model they developed.\n",
        "\n",
        "> In my experience in research and from this course, a good rule of thumb is taking the initial dataset given or provided and splitting it into two parts. The first part is what the programmer should train the model on to make sure it has an understanding of the type of input that it will process.  The second part is the testing set where the programmer will process the testing data into the model to see how it stands against new, randomized data.\n",
        "\n",
        "> Once both the training and test data are processed, it is important for a programmer to observe the difference in accuracy between the two sets and if there is any significant loss over the number of epochs the programmer has set.\n",
        "\n",
        "> One method of understanding the trained vs. test output is to develop a training and test loss graph that could show any overfitting or underfitting in the model.\n",
        "\n",
        "> Overfitting occurs when a model learns the training data to the extent that it results in a negative performance on new data. On the other hand, underfitting refers to data that cannot model the training data or generalize any new data that is being processed. Below is an example of overfitting and underfitting from my HW2 assignment.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DaY39NphZsvo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Importing dependencies\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "# Loading the MNIST Fashion Data set\n",
        "fashion_mnist = keras.datasets.fashion_mnist\n",
        "(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()\n",
        "\n",
        "# Preprocessing data\n",
        "train_images = train_images / 255.0\n",
        "test_images = test_images / 255.0\n",
        "\n",
        "# Setting up the 'Overfitting' model\n",
        "model = keras.Sequential([\n",
        "    keras.layers.Flatten(input_shape=(28,28)),\n",
        "    keras.layers.Dense(128*4, activation=tf.nn.relu),\n",
        "    keras.layers.Dense(10*4, activation=tf.nn.softmax)\n",
        "])\n",
        "\n",
        "# Compiling the model\n",
        "model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# train the model\n",
        "epochs = 20\n",
        "history = model.fit(train_images,\n",
        "                    train_labels,\n",
        "                    epochs=epochs,\n",
        "                    validation_data=(test_images, test_labels))\n",
        "\n",
        "test_loss, test_acc = model.evaluate(test_images, test_labels)\n",
        "history_dict = history.history\n",
        "acc_values = history_dict['acc']\n",
        "test_acc_values = history_dict['val_acc']\n",
        "loss_values = history_dict['loss']\n",
        "test_loss_values = history_dict['val_loss']\n",
        "epochs_range = range(1, epochs+1)\n",
        "print(\"Test Accuracy:\", test_acc)\n",
        "\n",
        "plt.plot(epochs_range, acc_values, 'bo', label='Training accuracy')\n",
        "plt.plot(epochs_range, test_acc_values, 'ro', label='Test accuracy')\n",
        "plt.title('Training and test accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Setting up the 'Underfitting' model\n",
        "model = keras.Sequential([\n",
        "    keras.layers.Flatten(input_shape=(28,28)),\n",
        "    keras.layers.Dense(1024, activation=tf.nn.relu),\n",
        "    keras.layers.Dropout(0.75),\n",
        "    keras.layers.Dense(10, activation=tf.nn.softmax)\n",
        "])\n",
        "\n",
        "# Compiling the model\n",
        "model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# train the model\n",
        "epochs = 20\n",
        "history = model.fit(train_images,\n",
        "                    train_labels,\n",
        "                    epochs=epochs,\n",
        "                    validation_data=(test_images, test_labels))\n",
        "\n",
        "test_loss, test_acc = model.evaluate(test_images, test_labels)\n",
        "history_dict = history.history\n",
        "acc_values = history_dict['acc']\n",
        "test_acc_values = history_dict['val_acc']\n",
        "loss_values = history_dict['loss']\n",
        "test_loss_values = history_dict['val_loss']\n",
        "epochs_range = range(1, epochs+1)\n",
        "print(\"Test Accuracy:\", test_acc)\n",
        "\n",
        "plt.plot(epochs_range, loss_values, 'bo', label='Training loss')\n",
        "plt.plot(epochs_range, test_loss_values, 'ro', label='Test loss')\n",
        "plt.title('Training and test loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4lWcWT9XaL6F",
        "colab_type": "text"
      },
      "source": [
        "### **Finetuning a pretrained model**\n",
        "\n",
        "When developing a training network, it is sometimes better to have a pretrained model to start off from rather than developing a network from scratch. The downside of developing a network from scratch, is that the programmer will need a rather large and expansive dataset to train their data on and a substantial amount of computing resources to be able to train the hypothetical data they use.\n",
        "\n",
        "The benefits of using a pretrained model is that you can load the pre-trained data into your code, freeze any of the required layers to avoid training on those layers, and then create your own model for the data to train on.\n",
        "\n",
        "Once the model is setup for training with the parameters you intend on using, you can now train your new model and check for the results to see how well your new model performed with the foundation of the pretrained model.\n"
      ]
    }
  ]
}